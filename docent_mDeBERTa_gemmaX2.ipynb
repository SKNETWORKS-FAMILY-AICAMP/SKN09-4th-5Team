{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uPYVJhy2ice0",
        "outputId": "5d90b182-be8c-4ec1-9a67-d2588b4bff9a"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip\n",
        "# !pip install --upgrade jupyter\n",
        "# !pip install --upgrade ipywidgets\n",
        "# !pip install \"numpy<2\"\n",
        "# !pip install pandas\n",
        "# !pip install torch\n",
        "# !pip install trl\n",
        "# !pip install peft\n",
        "# !pip install tqdm\n",
        "# !pip install idna\n",
        "# !pip install attr\n",
        "# !pip install aiohttp\n",
        "# !pip install typing\n",
        "# !pip install dotenv\n",
        "# !pip install requests\n",
        "# !pip install wikipedia\n",
        "# !pip install transformers\n",
        "# !pip install tokenizer\n",
        "# !pip install accelerate\n",
        "# !pip install sentence_transformers\n",
        "# !pip install scikit-learn\n",
        "# !pip install scipy\n",
        "# !pip install joblib\n",
        "# !pip install langchain==0.3.21\n",
        "# !pip install langchain-huggingface==0.1.2\n",
        "# !pip install langchain-community==0.3.20\n",
        "# !pip install langchain-core==0.3.51\n",
        "# !pip install langchain-openai==0.3.11\n",
        "# !pip install pydantic==2.7.4\n",
        "# !pip install faiss-cpu\n",
        "# !pip install faiss-gpu\n",
        "# !pip cache purge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, DebertaV2TokenizerFast, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import faiss\n",
        "import pydantic\n",
        "from abc import ABC\n",
        "from typing import Tuple, List, Dict, Optional, Any\n",
        "from peft import PeftModel, PeftConfig, PeftModelForQuestionAnswering\n",
        "from langchain.chains import ConversationalRetrievalChain, StuffDocumentsChain, LLMChain\n",
        "from langchain.chains.base import Chain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate, BasePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain.prompts.base import BasePromptTemplate\n",
        "from langchain.llms import BaseLLM, HuggingFacePipeline\n",
        "from langchain.agents import Tool, AgentExecutor, ZeroShotAgent\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "from langchain.tools import WikipediaQueryRun\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain_core.outputs import Generation, LLMResult\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.chat_models import ChatOpenAI      # Agent LLM API 활용할 때만 활성화 (비상시)\n",
        "# from google.colab import userdata                           # Agent LLM API 활용할 때만 활성화 (비상시)\n",
        "from dotenv import load_dotenv                              # Agent LLM API 활용할 때만 활성화 (비상시)\n",
        "load_dotenv()                                               # Agent LLM API 활용할 때만 활성화 (비상시)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ 병렬 토크나이저 경고 방지\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# ✅ 디바이스 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ✅ Q-LoRA 설정 로드\n",
        "peft_config = PeftConfig.from_pretrained(\"./trained_V3_LoRA\")\n",
        "\n",
        "# ✅ 기본 모델 로드\n",
        "loaded_model = AutoModelForQuestionAnswering.from_pretrained(peft_config.base_model_name_or_path)\n",
        "\n",
        "# ✅ Q-LoRA 어댑터 로드\n",
        "loaded_model = PeftModel.from_pretrained(loaded_model, \"./trained_V3_LoRA\")\n",
        "\n",
        "# ✅ 학습된 Q-LoRA 모델 및 토크나이저 로드\n",
        "loaded_model = loaded_model.to(device)\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./trained_V3_LoRA\")\n",
        "\n",
        "# ✅ 문장 임베딩 모델 로드 (LangChain 호환)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")  # snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
        "\n",
        "# ✅ 1️⃣ 데이터 로드 (박물관 데이터)\n",
        "data_path = './data/museum_data_rhys_250326.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df = df[[\"Title\", \"Description\"]].dropna().rename(columns={\"Title\": \"question\", \"Description\": \"answer\"})\n",
        "\n",
        "# ✅ 문장 분할 함수 (간단한 마침표, 물음표, 느낌표 기준 + 공백 제거)\n",
        "def split_sentences(text):\n",
        "    sentences = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
        "    return [s for s in sentences if s]\n",
        "\n",
        "documents = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    title = row[\"question\"]                 # Title 컬럼\n",
        "    description = row[\"answer\"]             # Description 컬럼\n",
        "    sentences = split_sentences(description)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        documents.append({\n",
        "            \"text\": f\"{title} - {sentence}\",\n",
        "            \"metadata\": {\"title\": title}\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ 2️⃣ FAISS 벡터스토어 생성 (LangChain)\n",
        "vectorstore = FAISS.from_texts(\n",
        "    texts=[doc[\"text\"] for doc in documents],\n",
        "    embedding=embedding_model,\n",
        "    metadatas=[doc[\"metadata\"] for doc in documents]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1548\\3790580037.py:27: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "# ✅ 벡터스토어의 상위 3개의 검색결과를 retriever로 저장\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# ✅ 3️⃣ Wikipedia 검색 도구 설정\n",
        "wiki_api = WikipediaAPIWrapper(lang=\"ko\")\n",
        "wikipedia_tool = WikipediaQueryRun(api_wrapper=wiki_api)\n",
        "\n",
        "# ✅ 4️⃣ 박물관 데이터 검색 도구 정의\n",
        "def search_museum_data(input):\n",
        "    docs = retriever.get_relevant_documents(input)\n",
        "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "# ✅ 5️⃣ 도구 목록 정의 (Agent가 사용할 도구)\n",
        "museum_tool = Tool(\n",
        "        name=\"Museum Data Search\",\n",
        "        func=search_museum_data,\n",
        "        description=\"This is useful when you need to answer questions about even the slightest of relevant information from the National Museum of Korea. Try searching first in the museum database. You need to input about even the slightest of relevant information from the museum. If you cannot find the appropriate answer in the database, try searching for Wikipedia. If you did not ask a question about the even the slightest of relevant information from the museum, you should specify to the user that it is not an appropriate question.\"\n",
        ")\n",
        "wiki_tool = Tool(\n",
        "        name=\"Wikipedia Search\",\n",
        "        func=wikipedia_tool.run,\n",
        "        description=\"This is useful if you have not found an appropriate answer to a user's question about the even the slightest of relevant information in the database. If you did not ask a question about the even the slightest of relevant information from the museum, you should specify to the user that it is not an appropriate question.\"\n",
        ")\n",
        "tools = [museum_tool, wiki_tool]\n",
        "\n",
        "# ✅ 6️⃣ 메모리 설정\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1548\\3072536880.py:24: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(\n"
          ]
        }
      ],
      "source": [
        "# ✅ 7️⃣ Agent에 사용할 LLM을 \"ModelSpace/GemmaX2-28-2B-v0.1\"로 정의 (AutoModelForCausalLM으로 호출)\n",
        "# control_model_id = \"ModelSpace/GemmaX2-28-2B-v0.1\"      # 모델을 \"ModelSpace/GemmaX2-28-2B-v0.1\"로 정의\n",
        "# control_tokenizer = AutoTokenizer.from_pretrained(control_model_id)\n",
        "# control_model = AutoModelForCausalLM.from_pretrained(control_model_id)\n",
        "\n",
        "# ✅ Langchain의 HuggingFacePipeline으로 \"ModelSpace/GemmaX2-28-2B-v0.1\"를 래핑\n",
        "# llm = HuggingFacePipeline(\n",
        "#     pipeline=pipeline(\n",
        "#         \"text-generation\",\n",
        "#         model=control_model,\n",
        "#         tokenizer=control_tokenizer,\n",
        "#         device=device,\n",
        "#         max_length=3072,        # 적절한 max_length 설정\n",
        "#         temperature=0.3,\n",
        "#         top_p=0.7,\n",
        "#         repetition_penalty=1.2,\n",
        "#         do_sample=True\n",
        "#     ),\n",
        "# )\n",
        "\n",
        "# ✅ Agent에 사용할 표준 LLM 코드 (gpt-4o-mini 사용)\n",
        "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "# openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "llm = ChatOpenAI(\n",
        "            api_key=openai_api_key,\n",
        "            model_name=\"gpt-4o-mini\",\n",
        "            temperature=0.3,\n",
        "            max_tokens=3072\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1548\\2351043044.py:21: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
            "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1548\\2351043044.py:24: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)       # verbose=True로 설정하면 Agent의 사고 과정을 볼 수 있습니다.\n"
          ]
        }
      ],
      "source": [
        "# ✅ 9️⃣ Agent 프롬프트 템플릿\n",
        "prefix = \"\"\"You are a useful agent designed to answer even the slightest of relevant questions about the National Museum of Korea. You have to find the words that match the information in the museum database from the user's questions and generate answers. To do this, you can access the following tools. Carefully review your questions and descriptions of the tools available to determine which tools are most appropriate to use. You have to try using museum_tool first. If you can't find any the slightest of relevant information in museum_tool, you should use wiki_tool. If you need a tool, clarify which tools you will use and provide accurate inputs for that tool. If you are able to answer questions without using the tool, please provide a direct answer.\n",
        "\n",
        "Available tools:\"\"\"\n",
        "suffix = \"\"\"Previous conversation: {chat_history}\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = ZeroShotAgent.create_prompt(\n",
        "    tools,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"]\n",
        ")\n",
        "\n",
        "# ✅ 1️⃣0️⃣ LLMChain 생성\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# ✅ 1️⃣1️⃣ Agent 생성\n",
        "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)       # verbose=True로 설정하면 Agent의 사고 과정을 볼 수 있습니다.\n",
        "\n",
        "# ✅ 1️⃣2️⃣ AgentExecutor 생성\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, verbose=True, memory=memory, handle_parsing_errors=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ 1️⃣3️⃣ 사용자 정의 LLM 래퍼 (파인튜닝된 모델 통합) - ConversationalRetrievalChain에 사용\n",
        "class CustomQAmodel(BaseLLM, Runnable):\n",
        "    model: PeftModelForQuestionAnswering        # 타입 힌트 객체 지정\n",
        "    tokenizer: DebertaV2TokenizerFast           # 타입 힌트 객체 지정\n",
        "    device: str\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom_qa_model\"\n",
        "\n",
        "    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None) -> LLMResult:\n",
        "        generations = []\n",
        "        for prompt in prompts:  # 입력된 모든 프롬프트에 대해 반복 처리\n",
        "            # print(f\"Prompt: {prompt}\")      # 모델 프롬프트 형태를 확인하기 위해 추가\n",
        "            try:\n",
        "                context_match = re.search(r\"Context:\\n(.*?)\\nQuestion:\", prompt, re.DOTALL)\n",
        "                question_match = re.search(r\"Question:\\n(.*?)(?:\\nPrevious conversation:(.*?))?\\nAnswer:\", prompt, re.DOTALL)\n",
        "\n",
        "                if not context_match or not question_match:\n",
        "                    generations.append([Generation(text=\"unknown\")]) # 또는 \"not sure\" 등 메인 루프에서 감지할 수 있는 키워드\n",
        "                    continue\n",
        "\n",
        "                context_part = context_match.group(1).strip()\n",
        "                # print(f\"Context Part: {context_part}\")              # 추출된 내용 확인을 위한 디버깅 코드\n",
        "\n",
        "                question_part = question_match.group(1).strip() if question_match.group(1) else question_match.group(3).strip()     # Question 파싱\n",
        "                # print(f\"Question Part: {question_part}\")            # 추출된 내용 확인을 위한 디버깅 코드\n",
        "\n",
        "                # chat_history 추출하고, chat_history가 없을 경우 빈 문자열 처리\n",
        "                chat_history_part = question_match.group(2).strip() if question_match.group(2) else (question_match.group(4).strip() if question_match.group(4) else \"\")\n",
        "                # print(f\"Chat History Part: {chat_history_part}\")    # 추출된 내용 확인을 위한 디버깅 코드\n",
        "\n",
        "            except Exception as e:\n",
        "                generations.append([Generation(text=f\"Error parsing prompt: {e}\")])\n",
        "                continue\n",
        "\n",
        "            # 추출한 chat_history를 모델 입력에 포함하는 방식 결정\n",
        "            # 예시: context와 question 앞에 chat_history를 추가하여 입력\n",
        "            augmented_input = f\"{chat_history_part}\\n{context_part}\\n{question_part}\"\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                augmented_input,\n",
        "                truncation=\"only_second\",\n",
        "                max_length=2048,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(self.device)\n",
        "\n",
        "            # print(f\"Inputs: {inputs}\")                  # 모델 입력 형태를 확인하기 위해 추가\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # print(f\"Outputs: {outputs}\")                # 모델 출력 형태를 확인하기 위해 추가\n",
        "\n",
        "            answer_start_scores = outputs.start_logits\n",
        "            answer_end_scores = outputs.end_logits\n",
        "            answer_start = torch.argmax(answer_start_scores)\n",
        "            answer_end = torch.argmax(answer_end_scores) + 1\n",
        "            answer = self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
        "\n",
        "            # print(f\"Answer: {answer}\")                  # 모델 정답 형태를 확인하기 위해 추가\n",
        "\n",
        "            generations.append([Generation(text=answer.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip())])\n",
        "\n",
        "            # print(f\"Prompt: {prompt}\")                  # 모델 프롬프트 형태를 확인하기 위해 추가\n",
        "\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        return self._generate([prompt], stop=stop)[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ 1️⃣4️⃣ 프롬프트 템플릿 (LangChain) - ConversationalRetrievalChain을 사용\n",
        "prompt_template = \"\"\"{prompt}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Previous conversation: {chat_history}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\", \"chat_history\", \"prompt\"])\n",
        "\n",
        "system_rules = \"\"\"\n",
        "\n",
        "<SYSTEM_RULE>\n",
        "\n",
        "1. NEVER mention that you're an AI. You are rather going to play a role as a docent of the National Museum of Korea in South Korea. You have to find the words that match the information in the museum's database from the user's questions, generate answers, and politely explain them to the user.\n",
        "\n",
        "2. Sense which country's language is the language of the user's question and you have to answer it in the language of the same country.\n",
        "\n",
        "3. Refrain from disclaimers about you not being a professional or expert.\n",
        "\n",
        "4. Keep responses unique and free of repetition.\n",
        "\n",
        "5. Answer based on the relevant information learned. If the relevant information is insufficient or does not exist in the file, please refer to RAG (search-based response) for reliable information.\n",
        "\n",
        "6. Always focus on the key points in user's questions to determine user's intent.\n",
        "\n",
        "7. Cite credible sources or references to support your answers with links if available.\n",
        "\n",
        "8. If a mistake is made in a previous response, recognize and correct it.\n",
        "\n",
        "9. If you don't have enough evidence or are unsure of the information, state “unknown” or “not sure” instead of making it up.\n",
        "\n",
        "10. Use only the information you know for certain to create a concise answer. If you have to make a guess, say “this is a guess”.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ✅ 1️⃣5️⃣ ConversationalRetrievalChain 설정\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=CustomQAmodel(model=loaded_model, tokenizer=loaded_tokenizer, device=device),\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    chain_type=\"stuff\",\n",
        "    condense_question_llm=llm,  # 질문 재구성을 위한 LLM 설정\n",
        "    combine_docs_chain_kwargs={\"prompt\": PROMPT.partial(prompt=system_rules)}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "POSp_ZnmiN8r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "질문: 겨울 산수에 대해서 알려줘\n",
            "답변: unknown\n",
            "구체적인 답변은 다음과 같습니다.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1548\\303115397.py:36: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result_agent = agent_executor.run({\"input\": question})             # Agent 실행 시 chat_history는 메모리에서 관리\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: The question is asking for information about \"겨울 산수\" (Winter Landscape), which may refer to a specific artwork or theme related to winter landscapes in Korean art. I will first search the museum database to see if there is any relevant information about this topic.  \n",
            "Action: Museum Data Search  \n",
            "Action Input: \"겨울 산수\"  \u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m겨울 산수 - 이 작품에서는 간략한 필치와 단순한 형태, 엷지만 선명한 색채가 두드러진다.\n",
            "\n",
            "겨울 산수 (冬景山水圖) - 이 작품에서는 간략한 필치와 단순한 형태, 엷지만 선명한 색채가 두드러진다.\n",
            "\n",
            "겨울 산수 - 산과 바위를 연한 먹빛으로 칠하고 태점(苔點 : 산이나 바위, 땅 또는 나무 줄기에 난 이끼를 표현하는 작은 점)을 찍어 장식적 효과를 높이고 있다.\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1548\\3790580037.py:10: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(input)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
            "Final Answer: \"겨울 산수\" (Winter Landscape) is characterized by a simple brushstroke and form, with light yet vivid colors. In this artwork, mountains and rocks are painted in a light ink tone, and decorative effects are enhanced by adding small dots to represent moss on the mountains, rocks, or trees.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "질문: 겨울 산수에 대해서 알려줘\n",
            "답변: \"겨울 산수\" (Winter Landscape) is characterized by a simple brushstroke and form, with light yet vivid colors. In this artwork, mountains and rocks are painted in a light ink tone, and decorative effects are enhanced by adding small dots to represent moss on the mountains, rocks, or trees.\n",
            "\n",
            "\n",
            "질문: 겨울 산수에 대해서 한국말로 알려줘\n",
            "답변: unknown\n",
            "구체적인 답변은 다음과 같습니다.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The user is asking for information about \"겨울 산수\" (Winter Landscape) in Korean. I need to check the museum database for relevant information about this artwork. \n",
            "Action: Museum Data Search\n",
            "Action Input: \"겨울 산수\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m겨울 산수 - 이 작품에서는 간략한 필치와 단순한 형태, 엷지만 선명한 색채가 두드러진다.\n",
            "\n",
            "겨울 산수 (冬景山水圖) - 이 작품에서는 간략한 필치와 단순한 형태, 엷지만 선명한 색채가 두드러진다.\n",
            "\n",
            "겨울 산수 - 산과 바위를 연한 먹빛으로 칠하고 태점(苔點 : 산이나 바위, 땅 또는 나무 줄기에 난 이끼를 표현하는 작은 점)을 찍어 장식적 효과를 높이고 있다.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
            "Final Answer: \"겨울 산수\" (Winter Landscape) 작품에서는 간략한 필치와 단순한 형태, 엷지만 선명한 색채가 두드러집니다. 이 작품에서는 산과 바위를 연한 먹빛으로 칠하고, 태점(苔點)이라는 작은 점을 찍어 장식적 효과를 높이고 있습니다.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "질문: 겨울 산수에 대해서 한국말로 알려줘\n",
            "답변: \"겨울 산수\" (Winter Landscape) 작품에서는 간략한 필치와 단순한 형태, 엷지만 선명한 색채가 두드러집니다. 이 작품에서는 산과 바위를 연한 먹빛으로 칠하고, 태점(苔點)이라는 작은 점을 찍어 장식적 효과를 높이고 있습니다.\n",
            "\n",
            "\n",
            "도슨트 서비스를 종료합니다. 이용해주셔서 고맙습니다.\n"
          ]
        }
      ],
      "source": [
        "# ✅ 1️⃣6️⃣ 답변 생성 단락\n",
        "while True:\n",
        "    # print(\"While loop is running\")                                  # 추출된 내용 확인을 위한 디버깅 코드\n",
        "    question = input(\"궁금한 점을 질문해주세요 (종료하려면 'exit' 입력): \")\n",
        "    if question.lower() == 'exit':\n",
        "        print(\"도슨트 서비스를 종료합니다. 이용해주셔서 고맙습니다.\")\n",
        "        break\n",
        "\n",
        "    # ConversationalRetrievalChain을 사용하여 답변 시도\n",
        "    # print(\"Invoking ConversationalRetrievalChain...\")               # 추출된 내용 확인을 위한 디버깅 코드\n",
        "    result_qa = qa.invoke({\"question\": question})\n",
        "    answer = result_qa[\"answer\"]\n",
        "\n",
        "    # ✅ context_part, question_part, chat_history_part 변수를 CustomQAmodel 내부에서 가져오기\n",
        "    context_part = result_qa.get(\"context_part\", \"Not Found\")  # context_part가 없으면 \"Not Found\" 출력\n",
        "    question_part = result_qa.get(\"question_part\", \"Not Found\")  # question_part가 없으면 \"Not Found\" 출력\n",
        "    chat_history_part = result_qa.get(\"chat_history_part\", \"Not Found\")  # chat_history_part가 없으면 \"Not Found\" 출력\n",
        "    # print(f\"CustomQAmodel Context Part: {context_part}\")            # 추출된 내용 확인을 위한 디버깅 코드\n",
        "    # print(f\"CustomQAmodel Question Part: {question_part}\")          # 추출된 내용 확인을 위한 디버깅 코드\n",
        "    # print(f\"CustomQAmodel Chat History Part: {chat_history_part}\")  # 추출된 내용 확인을 위한 디버깅 코드\n",
        "\n",
        "    # ✅ query / question / answer 출력 문제 파악을 위한 디버깅 코드\n",
        "    # print(f\"prompt: {prompt}\")                                  # 모델 출력 형태를 확인하기 위해 추가\n",
        "    # print(f\"qa: {qa}\")                                          # 모델 출력 형태를 확인하기 위해 추가\n",
        "    # print(f\"qa.input_keys: {qa.input_keys}\")                    # 모델 출력 형태를 확인하기 위해 추가\n",
        "    # print(f\"result_qa: {result_qa}\")                            # 모델 출력 형태를 확인하기 위해 추가\n",
        "    # print(f\"answer.split(): {answer.split()}\")                  # 모델 출력 형태를 확인하기 위해 추가\n",
        "    # print(f\"len(answer.split()): {len(answer.split())}\")        # 모델 출력 형태를 확인하기 위해 추가\n",
        "\n",
        "    print(f\"질문: {question}\")\n",
        "    print(f\"답변: {answer}\")\n",
        "\n",
        "    # 박물관 데이터에서 맥락을 찾지 못한 경우 (답변이 부실하거나 특정 키워드를 포함하는 경우), Agent 실행\n",
        "    if len(answer.split()) == 0 or \"구체적인 답변을 드리기 위해 검색을 시도합니다.\" in answer or \"unknown\" in answer.lower() or \"not sure\" in answer.lower():\n",
        "        print(\"구체적인 답변은 다음과 같습니다.\")\n",
        "        result_agent = agent_executor.run({\"input\": question})             # Agent 실행 시 chat_history는 메모리에서 관리\n",
        "        print(f\"질문: {question}\")\n",
        "        print(f\"답변: {result_agent}\")\n",
        "    else:\n",
        "        print(f\"질문: {question}\")\n",
        "        print(f\"답변: {answer}\")\n",
        "\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "vectordb_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
