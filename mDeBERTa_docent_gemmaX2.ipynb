{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uPYVJhy2ice0",
        "outputId": "5d90b182-be8c-4ec1-9a67-d2588b4bff9a"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy\n",
        "# !pip install pandas\n",
        "# !pip install torch\n",
        "# !pip install trl\n",
        "# !pip install peft\n",
        "# !pip install tqdm\n",
        "# !pip install idna\n",
        "# !pip install attr\n",
        "# !pip install aiohttp\n",
        "# !pip install typing\n",
        "# !pip install dotenv\n",
        "# !pip install requests\n",
        "# !pip install wikipedia\n",
        "# !pip install transformers\n",
        "# !pip install tokenizer\n",
        "# !pip install accelerate\n",
        "# !pip install sentence_transformers\n",
        "# !pip install scikit-learn\n",
        "# !pip install scipy\n",
        "# !pip install joblib\n",
        "# !pip install langchain==0.3.21\n",
        "# !pip install langchain-huggingface==0.1.2\n",
        "# !pip install langchain-community==0.3.20\n",
        "# !pip install langchain-core==0.3.49\n",
        "# !pip install langchain-openai==0.3.11\n",
        "# !pip install pydantic==2.7.4\n",
        "# !pip install faiss-cpu\n",
        "# !pip install faiss-gpu\n",
        "# !pip install --upgrade jupyter\n",
        "# !pip install --upgrade ipywidgets\n",
        "# !pip cache purge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, DebertaV2TokenizerFast, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import faiss\n",
        "import pydantic\n",
        "from abc import ABC\n",
        "from typing import Tuple, List, Dict, Optional, Any\n",
        "from peft import PeftModel, PeftConfig, PeftModelForQuestionAnswering\n",
        "from langchain.chains import ConversationalRetrievalChain, StuffDocumentsChain, LLMChain\n",
        "from langchain.chains.base import Chain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate, BasePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain.prompts.base import BasePromptTemplate\n",
        "from langchain.llms import BaseLLM, HuggingFacePipeline\n",
        "from langchain.agents import Tool, AgentExecutor, ZeroShotAgent\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "from langchain.tools import WikipediaQueryRun\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain_core.outputs import Generation, LLMResult\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "# from langchain_community.chat_models import ChatOpenAI      # Agent LLM API 활용할 때만 활성화 (비상시)\n",
        "# from google.colab import userdata                           # Agent LLM API 활용할 때만 활성화 (비상시)\n",
        "# from dotenv import load_dotenv                              # Agent LLM API 활용할 때만 활성화 (비상시)\n",
        "# load_dotenv()                                               # Agent LLM API 활용할 때만 활성화 (비상시)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ 병렬 토크나이저 경고 방지\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# ✅ 디바이스 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ✅ Q-LoRA 설정 로드\n",
        "peft_config = PeftConfig.from_pretrained(\"./trained_V3_LoRA\")\n",
        "\n",
        "# ✅ 기본 모델 로드\n",
        "loaded_model = AutoModelForQuestionAnswering.from_pretrained(peft_config.base_model_name_or_path)\n",
        "\n",
        "# ✅ Q-LoRA 어댑터 로드\n",
        "loaded_model = PeftModel.from_pretrained(loaded_model, \"./trained_V3_LoRA\")\n",
        "\n",
        "# ✅ 학습된 Q-LoRA 모델 및 토크나이저 로드\n",
        "loaded_model = loaded_model.to(device)\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./trained_V3_LoRA\")\n",
        "\n",
        "# ✅ 문장 임베딩 모델 로드 (LangChain 호환)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")  # snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
        "\n",
        "# ✅ 1️⃣ 데이터 로드 (박물관 데이터)\n",
        "data_path = './data/museum_data_rhys_250326.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df = df[[\"Title\", \"Description\"]].dropna().rename(columns={\"Title\": \"question\", \"Description\": \"answer\"})\n",
        "\n",
        "# ✅ 문장 분할 함수 (간단한 마침표, 물음표, 느낌표 기준 + 공백 제거)\n",
        "def split_sentences(text):\n",
        "    sentences = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
        "    return [s for s in sentences if s]\n",
        "\n",
        "documents = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    title = row[\"question\"]                 # Title 컬럼\n",
        "    description = row[\"answer\"]             # Description 컬럼\n",
        "    sentences = split_sentences(description)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        documents.append({\n",
        "            \"text\": f\"{title} - {sentence}\",\n",
        "            \"metadata\": {\"title\": title}\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ 2️⃣ FAISS 벡터스토어 생성 (LangChain)\n",
        "vectorstore = FAISS.from_texts(\n",
        "    texts=[doc[\"text\"] for doc in documents],\n",
        "    embedding=embedding_model,\n",
        "    metadatas=[doc[\"metadata\"] for doc in documents]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ 벡터스토어의 상위 3개의 검색결과를 retriever로 저장\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# ✅ 3️⃣ Wikipedia 검색 도구 설정\n",
        "wiki_api = WikipediaAPIWrapper(lang=\"ko\")\n",
        "wikipedia_tool = WikipediaQueryRun(api_wrapper=wiki_api)\n",
        "\n",
        "# ✅ 4️⃣ 박물관 데이터 검색 도구 정의\n",
        "def search_museum_data(input):\n",
        "    docs = retriever.get_relevant_documents(input)\n",
        "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "# ✅ 5️⃣ 도구 목록 정의 (Agent가 사용할 도구)\n",
        "museum_tool = Tool(\n",
        "        name=\"Museum Data Search\",\n",
        "        func=search_museum_data,\n",
        "        description=\"This is useful when you need to answer questions about even the slightest of relevant information from the National Museum of Korea. Try searching first in the museum database. You need to input about even the slightest of relevant information from the museum. If you cannot find the appropriate answer in the database, try searching for Wikipedia. If you did not ask a question about the even the slightest of relevant information from the museum, you should specify to the user that it is not an appropriate question.\"\n",
        ")\n",
        "wiki_tool = Tool(\n",
        "        name=\"Wikipedia Search\",\n",
        "        func=wikipedia_tool.run,\n",
        "        description=\"This is useful if you have not found an appropriate answer to a user's question about the even the slightest of relevant information in the database. If you did not ask a question about the even the slightest of relevant information from the museum, you should specify to the user that it is not an appropriate question.\"\n",
        ")\n",
        "tools = [museum_tool, wiki_tool]\n",
        "\n",
        "# ✅ 6️⃣ 메모리 설정\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ 7️⃣ Agent에 사용할 LLM을 \"ModelSpace/GemmaX2-28-2B-v0.1\"로 정의 (AutoModelForCausalLM으로 호출)\n",
        "control_model_id = \"ModelSpace/GemmaX2-28-2B-v0.1\"      # 모델을 \"ModelSpace/GemmaX2-28-2B-v0.1\"로 정의\n",
        "control_tokenizer = AutoTokenizer.from_pretrained(control_model_id)\n",
        "control_model = AutoModelForCausalLM.from_pretrained(control_model_id)\n",
        "\n",
        "# ✅ Langchain의 HuggingFacePipeline으로 \"ModelSpace/GemmaX2-28-2B-v0.1\"를 래핑\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=pipeline(\n",
        "        \"text-generation\",\n",
        "        model=control_model,\n",
        "        tokenizer=control_tokenizer,\n",
        "        device=device,\n",
        "        max_length=1024,        # 적절한 max_length 설정\n",
        "        temperature=0.3,\n",
        "        top_p=0.7,\n",
        "        repetition_penalty=1.2\n",
        "    ),\n",
        "    model_kwargs={\"do_sample\": True}\n",
        ")\n",
        "\n",
        "# ✅ Agent에 사용할 표준 LLM 코드 (gpt-4o-mini 사용)\n",
        "# openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "# openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "# llm = ChatOpenAI(\n",
        "#             api_key=openai_api_key,\n",
        "#             model_name=\"gpt-4o-mini\",\n",
        "#             temperature=0.3,\n",
        "#             max_tokens=1024\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ 9️⃣ Agent 프롬프트 템플릿\n",
        "prefix = \"\"\"You are a useful agent designed to answer even the slightest of relevant questions about the National Museum of Korea. You have to find the words that match the information in the museum database from the user's questions and generate answers. To do this, you can access the following tools. Carefully review your questions and descriptions of the tools available to determine which tools are most appropriate to use. You have to try using museum_tool first. If you can't find any the slightest of relevant information in museum_tool, you should use wiki_tool. If you need a tool, clarify which tools you will use and provide accurate inputs for that tool. If you are able to answer questions without using the tool, please provide a direct answer.\n",
        "\n",
        "Available tools:\"\"\"\n",
        "suffix = \"\"\"Previous conversation: {chat_history}\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = ZeroShotAgent.create_prompt(\n",
        "    tools,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"]\n",
        ")\n",
        "\n",
        "# ✅ 1️⃣0️⃣ LLMChain 생성\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# ✅ 1️⃣1️⃣ Agent 생성\n",
        "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)       # verbose=True로 설정하면 Agent의 사고 과정을 볼 수 있습니다.\n",
        "\n",
        "# ✅ 1️⃣2️⃣ AgentExecutor 생성\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, verbose=True, memory=memory, handle_parsing_errors=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ 1️⃣3️⃣ 사용자 정의 LLM 래퍼 (파인튜닝된 모델 통합) - ConversationalRetrievalChain에 사용\n",
        "class CustomQAmodel(BaseLLM, Runnable):\n",
        "    model: PeftModelForQuestionAnswering        # 타입 힌트 객체 지정\n",
        "    tokenizer: DebertaV2TokenizerFast           # 타입 힌트 객체 지정\n",
        "    device: str\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom_qa_model\"\n",
        "\n",
        "    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None) -> LLMResult:\n",
        "        generations = []\n",
        "        for prompt in prompts:  # 입력된 모든 프롬프트에 대해 반복 처리\n",
        "            # print(f\"Prompt: {prompt}\")      # 모델 프롬프트 형태를 확인하기 위해 추가\n",
        "            try:\n",
        "                context_match = re.search(r\"Context:\\n(.*?)\\nQuestion:\", prompt, re.DOTALL)\n",
        "                question_match = re.search(r\"Question:\\n(.*?)(?:\\nPrevious conversation:(.*?))?\\nAnswer:\", prompt, re.DOTALL)\n",
        "\n",
        "                if not context_match or not question_match:\n",
        "                    generations.append([Generation(text=\"unknown\")]) # 또는 \"not sure\" 등 메인 루프에서 감지할 수 있는 키워드\n",
        "                    continue\n",
        "\n",
        "                context_part = context_match.group(1).strip()\n",
        "                # print(f\"Context Part: {context_part}\")              # 추출된 내용 확인을 위한 디버깅 코드\n",
        "\n",
        "                question_part = question_match.group(1).strip() if question_match.group(1) else question_match.group(3).strip()     # Question 파싱\n",
        "                # print(f\"Question Part: {question_part}\")            # 추출된 내용 확인을 위한 디버깅 코드\n",
        "\n",
        "                # chat_history 추출하고, chat_history가 없을 경우 빈 문자열 처리\n",
        "                chat_history_part = question_match.group(2).strip() if question_match.group(2) else (question_match.group(4).strip() if question_match.group(4) else \"\")\n",
        "                # print(f\"Chat History Part: {chat_history_part}\")    # 추출된 내용 확인을 위한 디버깅 코드\n",
        "\n",
        "            except Exception as e:\n",
        "                generations.append([Generation(text=f\"Error parsing prompt: {e}\")])\n",
        "                continue\n",
        "\n",
        "            # 추출한 chat_history를 모델 입력에 포함하는 방식 결정\n",
        "            # 예시: context와 question 앞에 chat_history를 추가하여 입력\n",
        "            augmented_input = f\"{chat_history_part}\\n{context_part}\\n{question_part}\"\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                augmented_input,\n",
        "                truncation=\"only_second\",\n",
        "                max_length=3072,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(self.device)\n",
        "\n",
        "            # print(f\"Inputs: {inputs}\")                  # 모델 입력 형태를 확인하기 위해 추가\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # print(f\"Outputs: {outputs}\")                # 모델 출력 형태를 확인하기 위해 추가\n",
        "\n",
        "            answer_start_scores = outputs.start_logits\n",
        "            answer_end_scores = outputs.end_logits\n",
        "            answer_start = torch.argmax(answer_start_scores)\n",
        "            answer_end = torch.argmax(answer_end_scores) + 1\n",
        "            answer = self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
        "\n",
        "            # print(f\"Answer: {answer}\")                  # 모델 정답 형태를 확인하기 위해 추가\n",
        "\n",
        "            generations.append([Generation(text=answer.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip())])\n",
        "\n",
        "            # print(f\"Prompt: {prompt}\")                  # 모델 프롬프트 형태를 확인하기 위해 추가\n",
        "\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        return self._generate([prompt], stop=stop)[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ 1️⃣4️⃣ 프롬프트 템플릿 (LangChain) - ConversationalRetrievalChain을 사용\n",
        "prompt_template = \"\"\"{prompt}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Previous conversation: {chat_history}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\", \"chat_history\", \"prompt\"])\n",
        "\n",
        "system_rules = \"\"\"\n",
        "\n",
        "<SYSTEM_RULE>\n",
        "\n",
        "1. NEVER mention that you're an AI. You are rather going to play a role as a docent of the National Museum of Korea in South Korea. You have to find the words that match the information in the museum's database from the user's questions, generate answers, and politely explain them to the user.\n",
        "\n",
        "2. Sense which country's language is the language of the user's question and you have to answer it in the language of the same country.\n",
        "\n",
        "3. Refrain from disclaimers about you not being a professional or expert.\n",
        "\n",
        "4. Keep responses unique and free of repetition.\n",
        "\n",
        "5. Answer based on the relevant information learned. If the relevant information is insufficient or does not exist in the file, please refer to RAG (search-based response) for reliable information.\n",
        "\n",
        "6. Always focus on the key points in user's questions to determine user's intent.\n",
        "\n",
        "7. Cite credible sources or references to support your answers with links if available.\n",
        "\n",
        "8. If a mistake is made in a previous response, recognize and correct it.\n",
        "\n",
        "9. If you don't have enough evidence or are unsure of the information, state “unknown” or “not sure” instead of making it up.\n",
        "\n",
        "10. Use only the information you know for certain to create a concise answer. If you have to make a guess, say “this is a guess”.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ✅ 1️⃣5️⃣ ConversationalRetrievalChain 설정\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=CustomQAmodel(model=loaded_model, tokenizer=loaded_tokenizer, device=device),\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    chain_type=\"stuff\",\n",
        "    condense_question_llm=llm,  # 질문 재구성을 위한 LLM 설정\n",
        "    combine_docs_chain_kwargs={\"prompt\": PROMPT.partial(prompt=system_rules)}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POSp_ZnmiN8r"
      },
      "outputs": [],
      "source": [
        "# ✅ 1️⃣6️⃣ 답변 생성 단락\n",
        "while True:\n",
        "    # print(\"While loop is running\")                                  # 추출된 내용 확인을 위한 디버깅 코드\n",
        "    question = input(\"궁금한 점을 질문해주세요 (종료하려면 'exit' 입력): \")\n",
        "    if question.lower() == 'exit':\n",
        "        print(\"도슨트 서비스를 종료합니다. 이용해주셔서 고맙습니다.\")\n",
        "        break\n",
        "\n",
        "    # ConversationalRetrievalChain을 사용하여 답변 시도\n",
        "    # print(\"Invoking ConversationalRetrievalChain...\")               # 추출된 내용 확인을 위한 디버깅 코드\n",
        "    result_qa = qa.invoke({\"question\": question})\n",
        "    answer = result_qa[\"answer\"]\n",
        "\n",
        "    # ✅ context_part, question_part, chat_history_part 변수를 CustomQAmodel 내부에서 가져오기\n",
        "    context_part = result_qa.get(\"context_part\", \"Not Found\")  # context_part가 없으면 \"Not Found\" 출력\n",
        "    question_part = result_qa.get(\"question_part\", \"Not Found\")  # question_part가 없으면 \"Not Found\" 출력\n",
        "    chat_history_part = result_qa.get(\"chat_history_part\", \"Not Found\")  # chat_history_part가 없으면 \"Not Found\" 출력\n",
        "    # print(f\"CustomQAmodel Context Part: {context_part}\")            # 추출된 내용 확인을 위한 디버깅 코드\n",
        "    # print(f\"CustomQAmodel Question Part: {question_part}\")          # 추출된 내용 확인을 위한 디버깅 코드\n",
        "    # print(f\"CustomQAmodel Chat History Part: {chat_history_part}\")  # 추출된 내용 확인을 위한 디버깅 코드\n",
        "\n",
        "    # ✅ query / question / answer 출력 문제 파악을 위한 디버깅 코드\n",
        "    # print(f\"prompt: {prompt}\")                                  # 모델 출력 형태를 확인하기 위해 추가\n",
        "    # print(f\"qa: {qa}\")                                          # 모델 출력 형태를 확인하기 위해 추가\n",
        "    # print(f\"qa.input_keys: {qa.input_keys}\")                    # 모델 출력 형태를 확인하기 위해 추가\n",
        "    # print(f\"result_qa: {result_qa}\")                            # 모델 출력 형태를 확인하기 위해 추가\n",
        "    # print(f\"answer.split(): {answer.split()}\")                  # 모델 출력 형태를 확인하기 위해 추가\n",
        "    # print(f\"len(answer.split()): {len(answer.split())}\")        # 모델 출력 형태를 확인하기 위해 추가\n",
        "\n",
        "    print(f\"질문: {question}\")\n",
        "    print(f\"답변: {answer}\")\n",
        "\n",
        "    # 박물관 데이터에서 맥락을 찾지 못한 경우 (답변이 부실하거나 특정 키워드를 포함하는 경우), Agent 실행\n",
        "    if len(answer.split()) == 0 or \"구체적인 답변을 드리기 위해 검색을 시도합니다.\" in answer or \"unknown\" in answer.lower() or \"not sure\" in answer.lower():\n",
        "        print(\"구체적인 답변은 다음과 같습니다.\")\n",
        "        result_agent = agent_executor.run({\"input\": question})             # Agent 실행 시 chat_history는 메모리에서 관리\n",
        "        print(f\"질문: {question}\")\n",
        "        print(f\"답변: {result_agent}\")\n",
        "    else:\n",
        "        print(f\"질문: {question}\")\n",
        "        print(f\"답변: {answer}\")\n",
        "\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "vectordb_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
